server {
  http {
    port = ":8080"
  }
  thrift {
    port = ":8084"
  }
  admin {
    disable = true
  }
}

clients {
  caas {
    thrift {
      host = "di-user-profile"
      port = "8589"
      timeout_sec = 5
      client_id = "caas-client-from-bi-service"
    }
  }
  schema {
    thrift {
      host = "di-ingestion-service"
      port = "8487"
      timeout_sec = 5
      client_id = "ingestion-service-from-bi-service"
    }
  }

  lake {
    thrift {
      host = "explorer-queryservice"
      port = "8050"
      timeout_sec = 5
      client_id = "lake-service-from-lake-runner"
    }
  }

  hadoop_file {
    thrift {
      host = "explorer-server"
      port = "8051"
      timeout_sec = 5
      client_id = "hadoop-file-service-from-common-backend-clients"
    }
  }

  notification {
    thrift {
      host = "rocket-bi.ddns.net"
      port = "9999"
      timeout_sec = 5
      client_id = "notification-service-from-common-backend-clients"
    }
  }
}

database {
  mysql {
    url = "jdbc:mysql://rocket-mysql:3306?useUnicode=yes&characterEncoding=UTF-8&useLegacyDatetimeCode=false&serverTimezone=Asia/Ho_Chi_Minh"
    user = root
    password = "di@2020!"
  }
  clickhouse {
    url = "jdbc:clickhouse://clickhouse01:8123"
    user = "default"
    password = ""
    cluster_name = ""

    encryption {
      mode = "aes-256-gcm"
      key = "2f17958458160187530dcdbdc0ce892fb67a18100733ec35b0f713a5790b765d"
      iv = "d4fcb696b6e8e06b4a3cdc630e8176b7"
    }
  }
}

database_schema {
  database {
    name = "bi_service_schema"
  }
  table {
    dashboard {
      name = "dashboard"
      fields = ["org_id", "id", "name", "main_date_filter", "widgets", "widget_positions", "creator_id", "owner_id", "boost_info", "setting", "use_as_template"]
    }
    dashboard_field {
      name = "dashboard_field"
      fields = ["field_id", "dashboard_id", "field"]
    }
    directory {
      name = "directory"
      fields = ["org_id", "id", "name", "creator_id", "owner_id", "created_date", "parent_id", "is_removed", "dir_type", "dashboard_id", "updated_date", "data"]
    }

    permission_token {
      name = "permission_token"
      fields = ["token_id", "creator", "permissions", "created_time"]
    }

    object_sharing_token {
      name = "object_sharing_token"
      fields = ["object_type", "object_id", "token_id"]
    }

    geolocation {
      name = "geolocation"
      fields = ["code", "name", "normalized_name", "type", "latitude", "longitude", "properties"]
    }
    share_info {
      name = "share_info"
      fields = ["id", "organization_id", "resource_type", "resource_id", "username", "created_at", "updated_at", "created_by", "is_deleted"]
    }
    deleted_directory {
      name = "deleted_directory"
      fields = ["org_id", "id", "name", "creator_id", "owner_id", "created_date", "parent_id", "is_removed", "dir_type", "dashboard_id", "deleted_date", "updated_date"]
    }
    starred_directory {
      name = "starred_directory"
      fields = ["organization_id", "username", "directory_id"]
    }
    recent_directory {
      name = "recent_directory"
      fields = ["organization_id", "username", "directory_id", "last_seen_time"]
    }
    cohort {
      name = "cohort"
      fields = ["id", "name", "description", "organization_id", "creator_id", "owner_id", "created_time", "updated_time", "cohort"]
    }
    rls_policy {
      name = "rls_policy"
      fields = ["policy_id", "org_id", "user_ids", "user_attribute", "db_name", "tbl_name", "conditions"]
    }
  }
}

database_schema_testing {
  database {
    name = "bi_service_schema_test"
  }
}

ssdb {
  config {
    host = rocket-ssdb
    port = 8888
    timeout_in_ms = 3000
  }
}

test_environment {
  ssdb {
    service_name = ssdb
    port = 8888
  }
  clickhouse {
    service_name = clickhouse
    http_interface_port = 8123
    native_interface_port = 9000
  }
  mysql {
    service_name = mysql
    port = 3306
  }
  postgres {
    service_name = postgres
    port = 5432
  }
  mssql {
    service_name = mssql
    port = 1433
  }
}

fake_data {
  database {
    name = "database_test"
    default {
      name = "database_test"
    }
    marketing {
      name = "1001_xshop"
    }
  }
  table {
    customers {
      name = "Customers"
    }
    orders {
      name = "Orders"
    }
    products {
      name = "Products"
    }
    gdp {
      name = "Gdp"
    }
    activities {
      name = "Activities"
    }
    user_activities {
      name = "UserActivities"
    }
    default {
      name = "CHARACTER_SETS"
    }
    student {
      name = "student"
    }
    customers {
      name = "Customers"
    }
    orders {
      name = "Orders"
    }
    products {
      name = "Products"
    }
    gdp {
      name = "Gdp"
    }
    marketing {
      name = "marketing"
    }
  }
}

directory {
  parent_depth = 3
  user_root_dir = "bi-service.user_root_dir"
}


analytics {
  tracking_db_prefix = "analytics_"
  report_analytics_db_prefix = "analytics_report_"

  user_profile_table_name = "profiles"
  event_table_name = "di_user_events"

  report_active_user_collection_tbl = "active_user_collection"
  report_active_user_tbl = "active_user_metrics"

  system_event_names = {
    "di_user_events": "Events",
    "di_session_created": "Session Created"
    "di_session_end": "Session"
    "di_screen_enter": "Screen Enter"
    "di_screen_exit": "Screen Exit"
    "di_pageview": "PageView"
  }

  activity_hide_events = ["di_session_created", "di_screen_enter", "di_screen_exit"]
}

text_similarity_score {
  value = 0.85
}

api_key_resolver {
  default_key = "c2c09332-14a1-4eb1-8964-2d85b2a561c8"
  username = "root"
}


tracking_client {
  max_queue_size = 1000
  event_batch_size = 500

  sleep_time_ms = 100
  max_wait_time_ms = 10000

  tracking_host = "http://di-event-tracking-mw:8080/tracking/warehouse/stream"
  tracking_api_key = "c2c09332-14a1-4eb1-8964-2d85b2a561c8"

  user_activities {
    db_name = "di_system"
    table_name = "user_activities"
  }
}

clickhouse_csv_writer {
  host = "clickhouse01"
  tcp_port = 9000
  username = "default"
  password = ""
}

license {
  server_host = "https://license.rocket.bi/api/billing"
}

# schema-service
admin_secret_key = "12345678"
service_key = 12345678

db {
  clickhouse {
    is_auto_refresh_schema = true
    refresh_schema_interval_ms = 3600000 //1hour
  }

  mysql {
    dbname = "ingestion_schema"
    job_info_tbl = "job_infos"
    share_info_tbl = "share_info"
    sync_info_tbl = "file_sync_info"
    sync_history_tbl = "file_sync_history"
  }
}

test_db {
  mysql {
    service_name = mysql
    host = "localhost"
    port = 3306
    username = "root"
    password = "di@2020!"
    dbname = "ingestion_schema_test"
    job_info_tbl = "job_infos"
    share_info_tbl = "share_info"
    url = "jdbc:mysql://localhost:3306?useLegacyDatetimeCode=false&serverTimezone=Asia/Ho_Chi_Minh"
  }
  ssdb {
    service_name = ssdb
    port = 8888
  }
  clickhouse {
    service_name = clickhouse
    http_interface_port = 8123
    native_interface_port = 9000
  }
  postgres {
    service_name = "postgres"
    service_port = 5432
    url = "jdbc:postgresql://localhost:5432/thien_vi"
    username = "tvc12"
    password = "di@123456"
  }
  mssql {
    service_name = "mssql"
    service_port = 1433
    url = "jdbc:sqlserver://localhost:1433;databaseName=master"
    username = "sa"
    password = "di@123456"
    db_name = "thien_vi"
  }
  vertica {
    service_name = "vertica"
    service_port = 5433
    username = "vertica"
    password = "di@2020!"
  }


  oracle {
    url = ""
    username = ""
    password = ""
  }

  redshift {
    url = ""
    username = ""
    password = ""
  }
  mongodb {
    host = ""
    username = ""
    password = ""
  }
  shopify {
    api_url = "https://dev-datainsider.myshopify.com/"
    access_token = ""
    api_version = "2022-04"
  }
  coin_market_cap {
    api_key = ""
  }
}


schema {
  test {
    dbname = "job_scheduler_schema_test"
  }
  live {
    dbname = "job_scheduler_schema"
  }
  table {
    job {
      name = "job"
      fields = ["id", "name", "job_type", "creator_id", "last_modified", "sync_mode", "source_id", "last_successful_sync", "sync_interval_in_mn", "last_sync_status", "current_sync_status", "destination_db", "destination_tbl", "job_data", "organization_id", "next_run_time"]
    }
    source {
      name = "source"
      fields = ["id", "name", "data_source_type", "data_source_config", "organization_id", "creator_id", "last_modified"]
    }
    history {
      name = "history"
      fields = ["id", "job_id", "job_name", "last_sync_time", "total_sync_time", "sync_status", "total_inserted_rows", "message", "organization_id"]
    }

    lake_job {
      name = "lake_job"
      fields = ["id", "name", "job_type", "last_run_time", "last_run_status", "current_job_status", "job_data", "organization_id", "next_run_time", "creator_id"]
    }
    lake_history {
      name = "lake_history"
      fields = ["organization_id", "id", "job_id", "job_name", "yarn_app_id", "start_time", "end_time", "updated_time", "job_status", "message"]
    }

    tool_job {
      name = "tool_job"
      fields = ["job_id", "org_id", "name", "description", "job_type", "job_data", "schedule_time", "last_run_time", "last_run_status", "next_run_time", "current_run_status", "created_by", "created_at", "updated_by", "updated_at"]
    }
    tool_history {
      name = "tool_history"
      fields = ["run_id", "org_id", "job_id", "job_name", "job_type", "job_status", "job_data", "history_data", "begin_at", "end_at", "message"]
    }

  }
}

schedule_service {
  enable = true
  db_name = "interact_scheduler_worker"
  access_token = "job$cheduler@datainsider.co"
}

history_service {
  hadoop_base_log_dir = "/app-logs/root/logs-tfile"
}

# caas-service config
verification {
  email {
    send_grid {
      api_key = ""
    }
    sender = ""
    verify_link_host = "http://dev.datainsider.co/api"
    login_url = "http://dev.datainsider.co/login"
    password_reset_title = "DataInsider Password Changed"
    email_title = "DataInsider Verification Code"
    email_forgot_password_title = "DataInsider Forgot Password Verification Code"
    forgot_email_title = "DataInsider Forgot Password Verification Code"
    forgot_password_message_template = "Password Verification Code: $code"
    code_expire_time_in_second = 300
    limit_quota = 3
    limit_countdown_in_second = 180
    default_test_code = "123456"
    verification_enabled = true
  }
  phone {
    enabled {
      facebook = false
      u_p = true
      google = false
    }

    message_template = "DataInsider Verification Code: $code"
    forgot_password_message_template = "Password Verification Code: $code"
    code_expire_time_in_second = 180
    limit_quota = 3
    limit_countdown_in_second = 300
    default_test_code = "123456"
  }
}

session {
  authorization = "Authorization"
  domain = ".datainsider.co"
  name = "ssid"
  timeout_in_ms = 31104000000
}

cloudflare {
  main_domain = ""
  host = "https://api.cloudflare.com/client/v4"
  zone_id = ""
  api_key = ""

  sub_domain_config {
    type = "A"
    content = ""
    ttl = 1,
    priority = 10
    proxied = true
  }
}

recaptcha {
  host = "https://www.google.com/recaptcha/api/siteverify"
  secret = ""
}

oauth {
  whitelist_email_regex_pattern {
    default = ""
    gg = ""
    fb = ""
  }
  pass_secret = ""

  supported_methods = [
    {
      oauth_type = "gg"
      client_ids = [""]
    },
    {
      oauth_type = "fb"
      app_secret = ""
    }
  ]
}

#################### DataCook ####################

data_cook {
  query_size = 10000
  insert_batch_size = 100000
  sleep_interval_ms = 15000
  job_queue_size = 8
  num_job_worker = 4
  preview_prefix_db_name = "preview_etl"
  prefix_db_name = "etl"
  remove_preview_etl_data_interval_minutes = 30
  connection_timeout_in_second = 60
  running_job_db = "data_cook.running_job"
  running_job_db_test = "data_cook.running_job_test"
  ssl_dir = "./tmp/ssl"
  mail_dir = "./tmp/email"
  tmp_dir = "./tmp"
  python_execute_timeout = 7200000 //2 hours

  clickhouse {
    host = "clickhouse01"
    port = "9000"
    http_port = "8123"
    user = "default"
    password = ""
  }

  engine_cache {
    expire_time_in_second = 7200
    max_size = 500
  }

  mysql {
    host = "rocket-mysql"
    port = 3306
    username = "root"
    password = "di@2020!"
    dbname = "etl"

    job_table = "job"
    deleted_table = "deleted_job"
    job_history_table = "job_history"
    share_table = "share_info"
  }

  test_mysql {
    host = "localhost"
    port = 3306
    username = "root"
    password = "di@2020!"
    dbname = "etl_test"

    job_table = "job"
    deleted_table = "deleted_job"
    job_history_table = "job_history"
    share_table = "share_info"
  }

  jdbc_test {
    oracle {
      host = "localhost"
      port = 1521
      service_name = "ORCLCDB.localdomain"
      username = "TVC12"
      password = "di@123456"
      dbname = "TVC12"
    }
    mysql {
      host = "localhost"
      port = 3306
      username = "root"
      password = "di@2020!"
      dbname = "persist_test"
    }
    mssql {
      host = "localhost"
      port = 1433
      username = "sa"
      password = "di@123456"
      catalog = "thien_vi"
    }
    postgres {
      host = "localhost"
      port = 5432
      username = "tvc12"
      password = "di@123456"
      catalog = "thien_vi"
    }
  }

  send_grid {
    api_key = ""
    sender = ""
    sender_name = "DataInsider Export"
    rate_limit_retry = 5
    sleep_in_mills = 5000
    limit_size_in_bytes = 30000000
  }
  templates {
    python = "template/main.py.template"
  }
}

###################### start job-worker config ######################

jobworker {
  scheduler_host = "http://localhost:8080"
  job_queue_size = 8
  num_job_worker = 4
  sleep_interval_ms = 10000
  access_token = "job$cheduler@datainsider.co"

}

google {
  gg_client_id = ""
  gg_client_secret = ""
  server_encoded_url = "https://accounts.google.com/o/oauth2/token"
  redirect_uri = "https%3A%2F%2Fhello.datainsider.co"
  read_timeout_ms = 300000
  connection_timeout_ms = 300000
  batch_size = 100000
  application_name = "Data Insider"
}

kafka-hadoop-data-block-config {

  topic = "kafka-hadoop-data-block-dev"

  delay-poll = 100    // delay between polls. 100 millis
  delay-error = 5000  // when consume error, delay and retry. 5 seconds

  producer {
    bootstrap.servers = "di-kafka:29092"
    acks = "all"
    linger.ms = 1
  }

}

hadoop_destination {
  block_chunk_size = 100
  base_dir = "/data/db"
  trash_path = "/user/root/.Trash"
}

hadoop-writer {
  base_dir = "/data/db"
  local-file-writer {
    base_dir = "./tmp/hadoop"
    file_extension = "txt"
    max_file_size = 64 MB
    max_queue_size = 10000
    enqueue_timeout_ms = 100
  }
  hdfs {
    file_system = "hdfs://namenode:9000"
    app_name = "Spark-file-service"
    master = "local[*]"
    num_partitions = 1
  }
}

solana {
  max_queue_size = 500
}


coin_market_cap {
  host = "https://pro-api.coinmarketcap.com"
  query_size = 1000
}

amazon_s3_worker {
  sync_batch_size = 1000
  base_dir = "./tmp/s3"
  connection_timeout = 600000
  sample_bytes_for_preview = 1000000
}

shopify {
  retry_time_out_ms = 30000
  min_retry_time_delay_ms = 500
  max_retry_time_delay_ms = 1000
  client_id = ""
  client_secret = ""
}


google_ads_api {
  gg_client_id = ""

  gg_client_secret = ""

  server_encoded_url = "https://accounts.google.com/o/oauth2/token"

  redirect_uri = "http%3A%2F%2Flocalhost%3A8080&"

  developer_token = ""

  metadata_uri = "https://gaql-query-builder.uc.r.appspot.com/schemas/v12"

  batch_size = 1000

  default_start_time = "2019-01-01"

}

worker_v2 {
  write_batch_size = 5000
  report_interval_size = 100000
}

ga4 {
  batch_size = 10000
}

facebook_ads {
  app_id = ""
  app_secret = ""
  exchange_url = "https://graph.facebook.com/v15.0/oauth/access_token"
}

tiktok_ads {
  app_key = ""
  app_secret = ""
  base_url = "https://business-api.tiktok.com/open_api/v1.3"
}

shopee {
  api_url = "https://partner.test-stable.shopeemobile.com"
  partner_id = ""
  partner_key = ""
}


palexy {
  base_url = ""
  window_days = 30
  retry_times = 3
  retry_interval = 1000
}


###################### end job-worker config ######################

clickhouse_engine {
  max_query_rows = 10000
  conn_timeout_ms = 180000
  client_pool_size = 10

  local_file_writer {
    base_dir = "./tmp/clickhouse"
    file_extension = "json"
    max_file_size = 1024 MB
  }
}

bigquery_engine {
  max_query_rows = 10000
  conn_timeout_ms = 180000

  local_file_writer {
    base_dir = "./tmp/bigquery"
    file_extension = "json"
    max_file_size = 1024 MB
  }

}

mysql_engine {
  max_query_rows = 10000
  client_pool_size = 10
  conn_timeout_ms = 180000
  insert_batch_size = 100000
}

vertica_engine {
  conn_timeout_ms = 30000
  client_pool_size = 10
  insert_batch_size = 100000
}

postgres_engine {
  conn_timeout_ms = 30000
  client_pool_size = 10
  max_query_rows = 10000
}

client_pool {
  size = 100
  expire_time_ms = 300000 // 5 minutes
}

slack_notification {
  webhook_url = ""
}

hidden_db_name_patterns = ["^(?:org\\d+_)?(?:preview_etl|etl)_\\d+$"]
